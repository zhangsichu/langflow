"use strict";(self.webpackChunklangflow_docs=self.webpackChunklangflow_docs||[]).push([[548],{17886:(e,n,s)=>{s.d(n,{A:()=>t});s(96540);var o=s(64058),i=s(74848);function t({name:e,...n}){const s=o[e];return s?(0,i.jsx)(s,{...n}):null}},38223:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>l,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"Concepts/concepts-voice-mode","title":"Use voice mode","description":"You can use Langflow\'s voice mode to interact with your flows verbally through a microphone and speakers.","source":"@site/docs/Concepts/concepts-voice-mode.mdx","sourceDirName":"Concepts","slug":"/concepts-voice-mode","permalink":"/concepts-voice-mode","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Use voice mode","slug":"/concepts-voice-mode"},"sidebar":"docs","previous":{"title":"Use Langflow data types","permalink":"/data-types"},"next":{"title":"Use the Langflow CLI","permalink":"/configuration-cli"}}');var i=s(74848),t=s(28453),r=s(17886);const l={title:"Use voice mode",slug:"/concepts-voice-mode"},a=void 0,c={},d=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Test voice mode in the Playground",id:"test-voice-mode-in-the-playground",level:2},{value:"Develop applications with websockets endpoints",id:"develop-applications-with-websockets-endpoints",level:2},{value:"Voice-to-voice audio streaming",id:"voice-to-voice-audio-streaming",level:3},{value:"Speech-to-text audio transcription",id:"speech-to-text-audio-transcription",level:3},{value:"Session IDs for websockets endpoints",id:"session-ids-for-websockets-endpoints",level:3},{value:"See also",id:"see-also",level:2}];function h(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.p,{children:"You can use Langflow's voice mode to interact with your flows verbally through a microphone and speakers."}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(n.p,{children:"Voice mode requires the following:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["A flow with ",(0,i.jsx)(n.strong,{children:"Chat Input"}),", ",(0,i.jsx)(n.strong,{children:"Language Model"}),", and ",(0,i.jsx)(n.strong,{children:"Chat Output"})," components."]}),"\n",(0,i.jsxs)(n.p,{children:["If your flow has an ",(0,i.jsx)(n.strong,{children:"Agent"})," component, make sure the tools in your flow have accurate names and descriptions to help the agent choose which tools to use."]}),"\n",(0,i.jsxs)(n.p,{children:["Additionally, be aware that voice mode overrides typed instructions in the ",(0,i.jsx)(n.strong,{children:"Agent"})," component's ",(0,i.jsx)(n.strong,{children:"Agent Instructions"})," field."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["An ",(0,i.jsx)(n.a,{href:"https://platform.openai.com/",children:"OpenAI"})," account and an OpenAI API key because Langflow uses the OpenAI API to process voice input and generate responses."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Optional: An ",(0,i.jsx)(n.a,{href:"https://elevenlabs.io",children:"ElevenLabs"})," API key to enable more voice options for the LLM's response."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"A microphone and speakers."}),"\n",(0,i.jsx)(n.p,{children:"A high quality microphone and minimal background noise are recommended for optimal voice comprehension."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"test-voice-mode-in-the-playground",children:"Test voice mode in the Playground"}),"\n",(0,i.jsxs)(n.p,{children:["In the ",(0,i.jsx)(n.strong,{children:"Playground"}),", click the ",(0,i.jsx)(r.A,{name:"Mic","aria-hidden":"true"})," ",(0,i.jsx)(n.strong,{children:"Microphone"})," to enable voice mode and verbally interact with your flows through a microphone and speakers."]}),"\n",(0,i.jsxs)(n.p,{children:["The following steps use the ",(0,i.jsx)(n.strong,{children:"Simple Agent"})," template to demonstrate how to enable voice mode:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Create a flow based on the ",(0,i.jsx)(n.strong,{children:"Simple Agent"})," template."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Add your ",(0,i.jsx)(n.strong,{children:"OpenAI API key"})," credentials to the ",(0,i.jsx)(n.strong,{children:"Agent"})," component."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Click ",(0,i.jsx)(n.strong,{children:"Playground"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Click the ",(0,i.jsx)(r.A,{name:"Mic","aria-hidden":"true"})," ",(0,i.jsx)(n.strong,{children:"Microphone"})," icon to open the ",(0,i.jsx)(n.strong,{children:"Voice mode"})," dialog."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Enter your OpenAI API key, and then click ",(0,i.jsx)(n.strong,{children:"Save"}),". Langflow saves the key as a ",(0,i.jsx)(n.a,{href:"/configuration-global-variables",children:"global variable"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"If you are prompted to grant microphone access, you must allow microphone access to use voice mode.\nIf microphone access is blocked, you won't be able to provide verbal input."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["For ",(0,i.jsx)(n.strong,{children:"Audio Input"}),", select the input device to use with voice mode."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Optional: Add an ElevenLabs API key to enable more voices for the LLM's response.\nLangflow saves this key as a global variable."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["For ",(0,i.jsx)(n.strong,{children:"Preferred Language"}),", select the language you want to use for your conversations with the LLM.\nThis option changes both the expected input language and the response language."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Speak into your microphone to start the chat."}),"\n",(0,i.jsxs)(n.p,{children:["If configured correctly, the waveform registers your input, and then the agent's logic and response are described verbally and in the ",(0,i.jsx)(n.strong,{children:"Playground"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"develop-applications-with-websockets-endpoints",children:"Develop applications with websockets endpoints"}),"\n",(0,i.jsxs)(n.p,{children:["Langflow exposes two OpenAI Realtime API-compatible websocket endpoints for your flows.\nYou can build applications against these endpoints the same way you would build against ",(0,i.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/realtime#connect-with-websockets",children:"OpenAI Realtime API websockets"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["The Langflow API's websockets endpoints require an ",(0,i.jsx)(n.a,{href:"https://platform.openai.com/docs/overview",children:"OpenAI API key"})," for authentication, and they support an optional ",(0,i.jsx)(n.a,{href:"https://elevenlabs.io",children:"ElevenLabs"})," integration with an ElevenLabs API key."]}),"\n",(0,i.jsx)(n.p,{children:"Additionally, both endpoints require that you provide the flow ID in the endpoint path."}),"\n",(0,i.jsx)(n.h3,{id:"voice-to-voice-audio-streaming",children:"Voice-to-voice audio streaming"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"/ws/flow_as_tool/$FLOW_ID"})," endpoint establishes a connection to OpenAI Realtime voice, and then invokes the specified flow as a tool according to the ",(0,i.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/realtime-conversations#handling-audio-with-websockets",children:"OpenAI Realtime model"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"This approach is ideal for low latency applications, but it is less deterministic because the OpenAI voice-to-voice model determines when to call your flow."}),"\n",(0,i.jsx)(n.h3,{id:"speech-to-text-audio-transcription",children:"Speech-to-text audio transcription"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"/ws/flow_tts/$FLOW_ID"})," endpoint converts audio to text using ",(0,i.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/realtime-transcription",children:"OpenAI Realtime voice transcription"}),", and then directly invokes the specified flow for each transcript."]}),"\n",(0,i.jsx)(n.p,{children:"This approach is more deterministic but has higher latency."}),"\n",(0,i.jsxs)(n.p,{children:["This is the mode used in the Langflow ",(0,i.jsx)(n.strong,{children:"Playground"}),"."]}),"\n",(0,i.jsx)(n.h3,{id:"session-ids-for-websockets-endpoints",children:"Session IDs for websockets endpoints"}),"\n",(0,i.jsxs)(n.p,{children:["Both endpoints accept an optional ",(0,i.jsx)(n.code,{children:"/$SESSION_ID"})," path parameter to provide a unique ID for the conversation.\nIf omitted, Langflow uses the flow ID as the ",(0,i.jsx)(n.a,{href:"/session-id",children:"session ID"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["However, be aware that voice mode only maintains context within the current conversation instance.\nWhen you close the ",(0,i.jsx)(n.strong,{children:"Playground"})," or end a chat, verbal chat history is discarded and not available for future chat sessions."]}),"\n",(0,i.jsx)(n.h2,{id:"see-also",children:"See also"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"/concepts-playground",children:"Test flows in the Playground"})}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}}}]);