"use strict";(self.webpackChunklangflow_docs=self.webpackChunklangflow_docs||[]).push([[6608],{28453:(n,e,i)=>{i.d(e,{R:()=>r,x:()=>l});var s=i(96540);const t={},o=s.createContext(t);function r(n){const e=s.useContext(o);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:r(n.components),s.createElement(o.Provider,{value:e},n.children)}},51768:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>d,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>a});const s=JSON.parse('{"id":"Integrations/Nvidia/integrations-nvidia-nim-wsl2","title":"Integrate NVIDIA NIMs with Langflow","description":"Connect Langflow with NVIDIA NIM on an RTX Windows system with Windows Subsystem for Linux 2 (WSL2) installed.","source":"@site/docs/Integrations/Nvidia/integrations-nvidia-nim-wsl2.mdx","sourceDirName":"Integrations/Nvidia","slug":"/integrations-nvidia-ingest-wsl2","permalink":"/integrations-nvidia-ingest-wsl2","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Integrate NVIDIA NIMs with Langflow","slug":"/integrations-nvidia-ingest-wsl2"},"sidebar":"docs","previous":{"title":"NVIDIA Ingest","permalink":"/integrations-nvidia-ingest"},"next":{"title":"NVIDIA G-Assist","permalink":"/integrations-nvidia-g-assist"}}');var t=i(74848),o=i(28453);const r={title:"Integrate NVIDIA NIMs with Langflow",slug:"/integrations-nvidia-ingest-wsl2"},l=void 0,d={},a=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Use the NVIDIA NIM in a flow",id:"use-the-nvidia-nim-in-a-flow",level:2}];function c(n){const e={a:"a",code:"code",h2:"h2",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.p,{children:["Connect Langflow with NVIDIA NIM on an RTX Windows system with ",(0,t.jsx)(e.a,{href:"https://learn.microsoft.com/en-us/windows/wsl/install",children:"Windows Subsystem for Linux 2 (WSL2)"})," installed."]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.a,{href:"https://docs.nvidia.com/nim/index.html",children:"NVIDIA NIM (NVIDIA Inference Microservices)"})," provides containers to self-host GPU-accelerated inferencing microservices.\nIn this example, you connect a Language Model component in Langflow to a deployed ",(0,t.jsx)(e.code,{children:"mistral-nemo-12b-instruct"})," NIM on an ",(0,t.jsx)(e.strong,{children:"RTX Windows system"})," with ",(0,t.jsx)(e.strong,{children:"WSL2"}),"."]}),"\n",(0,t.jsxs)(e.p,{children:["For more information on NVIDIA NIM, see the ",(0,t.jsx)(e.a,{href:"https://docs.nvidia.com/nim/index.html",children:"NVIDIA documentation"}),"."]}),"\n",(0,t.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.a,{href:"https://docs.nvidia.com/nim/wsl2/latest/getting-started.html",children:"NVIDIA NIM WSL2 installed"})}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:"A NIM container deployed according to the model's instructions"}),"\n",(0,t.jsxs)(e.p,{children:["Prerequisites vary between models.\nFor example, to deploy the ",(0,t.jsx)(e.code,{children:"mistral-nemo-12b-instruct"})," NIM, follow the instructions for ",(0,t.jsx)(e.strong,{children:"Windows on RTX AI PCs (Beta)"})," on your ",(0,t.jsx)(e.a,{href:"https://build.nvidia.com/nv-mistralai/mistral-nemo-12b-instruct/deploy?environment=wsl2.md",children:"model's deployment overview"}),"."]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:"Windows 11 build 23H2 or later"}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:"At least 12 GB of RAM"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"use-the-nvidia-nim-in-a-flow",children:"Use the NVIDIA NIM in a flow"}),"\n",(0,t.jsxs)(e.p,{children:["To connect the deployed NIM to Langflow, add the ",(0,t.jsx)(e.strong,{children:"NVIDIA"})," language model component to a flow:"]}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["Create a flow based on the ",(0,t.jsx)(e.strong,{children:"Basic Prompting"})," template."]}),"\n",(0,t.jsxs)(e.li,{children:["Replace the ",(0,t.jsx)(e.strong,{children:"OpenAI"})," model component with the ",(0,t.jsx)(e.strong,{children:"NVIDIA"})," component."]}),"\n",(0,t.jsxs)(e.li,{children:["In the ",(0,t.jsx)(e.strong,{children:"NVIDIA"})," component's ",(0,t.jsx)(e.strong,{children:"Base URL"})," field, add the URL where your NIM is accessible. If you followed your model's ",(0,t.jsx)(e.a,{href:"https://build.nvidia.com/nv-mistralai/mistral-nemo-12b-instruct/deploy?environment=wsl2.md",children:"deployment instructions"}),", the value is ",(0,t.jsx)(e.code,{children:"http://localhost:8000/v1"}),"."]}),"\n",(0,t.jsxs)(e.li,{children:["In the ",(0,t.jsx)(e.strong,{children:"NVIDIA"})," component's ",(0,t.jsx)(e.strong,{children:"NVIDIA API Key"})," field, add your NVIDIA API Key."]}),"\n",(0,t.jsxs)(e.li,{children:["Select your model from the ",(0,t.jsx)(e.strong,{children:"Model Name"})," field."]}),"\n",(0,t.jsxs)(e.li,{children:["Open the ",(0,t.jsx)(e.strong,{children:"Playground"})," and chat with your ",(0,t.jsx)(e.strong,{children:"NIM"})," model."]}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(c,{...n})}):c(n)}}}]);